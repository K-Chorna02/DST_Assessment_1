---
title: "Random Forest"
output: html_document
date: "2025-10-15"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load packages 


library(here)

library(dplyr)
 
library(tidyr) #for pivoting 

library(randomForest)


europe_data <- read.csv(here("GracieBelmonte","data","who_data.csv"))

europe_data <- europe_data %>%
  filter(who_region == "4_Eur") %>%
  select(-pm25_concentration) #we have excluded pm25 as it is too messy 

```

#1. Introduction

[NOTES:IntroduceRandom forests

What they are Why it is appropriate How this will help answer our
prediction goal of "predicting what the PM10 value will be in 2022"

How it works - discuss mathematics behind the algorithm- TO DO \~ talk
about maths behind boot strapping How it can handle missing values ]

**what are they** The use of decision trees is both simple and intuitive.
They provide a straight forward, visual way to represent how different
variables influence a certain outcome by splitting data into small
groups based on predictor values. However, single decision trees can be
unstable and sensitive to training data. Minor changes in the dataset
can lead to major changes in the predicted outcomes. Hence this makes
them inefficient when generalising new, unseen data . #mention
overfitting

Random Forest trees presents a way of overcoming these issues. The idea
is that they combine the predictions of many single decision trees into
one robust and accurate model. Instead of relying on a single tree, a
Random Forest builds multiple trees using different samples of the data
and random subsets of features at each split. The results from all trees
are then averaged (for regression) or voted on (for classification) to
generate the final prediction.

**why the are appropriate/ help predict** With this in mind, I can
confidently say that this model will be highly appropriate for our aim
of predicting PM10 concentrations in 2022 across European cities. Data
regarding air quality is complex. It can contain non-linear patterns,
interactions between pollutants and regional differences. A single
regression model may struggle to capture these elements, but Random
Forests can adapt to these complexities without requiring strong
assumptions about the data's structure.

reference: "<https://bradleyboehmke.github.io/HOML/random-forest.html>"

#2. data preparation

Indentify the years avaiable so we can isolate 2010-2020

maybe make a model that removes missing values for comparison - as an
extension

NEAT:

Before training the Random Forest model, we needed to pre-process the
data to ensure it was suitable for prediction. In this section we have:

-   Identified the available years and isolate the range 2010- 2021.
    (everyone has probs done this so not just for my section)
-   Created a **training set** (2010- 2020) and a **test set** (2021).
-   Handled missing values appropriately, following the Statology (2023)
    approach.

```{r}
#check which years are avaible 
unique(europe_data$year)

# Quick summary of counts per year
table(europe_data$year)

colnames(europe_data)
```

so we can confirm the range and train on 2010-2021, \# this will probs
go in the intro as we wil all be doing the same

```{r}
# filter relevent years and variables

# do i need to use minnies combined europe data instead?

europe_pm10 <- europe_data %>%
  filter(year >= 2010 & year <= 2021) %>%
  select(country_name, year, pm10_concentration, no2_concentration, population, latitude, longitude) 

#split training and test data

train_data <- europe_pm10 %>% filter( year < 2021)
test_data <- europe_pm10 %>% filter( year == 2021)

#replace missing values in predictors with column medians 
for (i in c("no2_concentration", "population")) {
  train_data[[i]][is.na(train_data[[i]])] <- median(train_data[[i]], na.rm = TRUE)
  test_data[[i]][is.na(test_data[[i]])] <- median(test_data[[i]], na.rm = TRUE)
}

#str(train_data)

```

### Summary of Prepartion Steps

-   The dataset was filtered to include only European countries and the
    years **2010-2021**, ensuring a consistent range for analysis.

The key variables retained were: 

- `pm10_concentration` - the target variable to be predicted. 
- `no2_concentration`, `population`, `longitude` and `latitude` - predictor variables.
- `country_name` and `year` - identifying variable.


The dataset was divided into:
    -   a **training set** (2010- 2020) used to build the model, and
    -   a **test set** (2021) used to evaluate its predictive
        performance.


**Missing predictor values** were replaced using median imputation,
following the method described in:
""<https://www.statology.org/random-forest-in-r/>""


We left the target variable pm10_concentration untouched, since that is
what we are predicting. (But we did have to remove it's missing values
next step so that Random Forest could acc work?)

**3. create inputs and targets**

I think this has all been covered in step above

#4. Train the Random Forest Model

This section implements the Random Forest algorithm, again using code
from: "<https://www.statology.org/random-forest-in-r/>"

```{r}
# random forest wont work with missing values in pm10?

train_data <- train_data %>% filter(!is.na(pm10_concentration))

#make this example reproducible - lock in the random number generators starting point 
set.seed(1)

#fit Random Forest model 
rf_model <- randomForest(
  formula = pm10_concentration ~ no2_concentration + population + latitude + longitude, 
  data = train_data
)

# view model summary 
rf_model 

```

### Summary of Model Implementation

-   The model was specified as:

**PM10 concentration** \~ **NOâ‚‚ concentration** + **population**

`pm10_concentration ~ no2_concentration + population`

-   The model was trained using **PM10 concentration** as the response
    variable, and **NO2 concentration** and **population** as predictor
    variables.
-   The Random Forest built **500 decision trees**, each trained on a
    random subset of the data and predictors.
-   Meaning the Random Forest algorithm combines the predictions of 500
    individual decision trees by averaging their outputs, a process
    known as **ensemble averaging**.
-   Thus this averaging process smooths out the variability from any one
    tree, reducing random noise and leading to a more accurate and
    stable model.

-The random seed 'set.seed(1)' ensures that results are reproducible and
so identical outcomes are produced each time the code is run.

-The model output shows: (maybe I dont need to mention this if using an
diff metric later)

- **Mean of Squared Residuals = 29.97915** -
representing the average squared difference between predicted and actual
PM10 values. 
- **% Variance Explained = 78.53%** - indicating that almost 80% of the variance in PM10 levels is explained by NO2
concentration, population, logitude and latitude. 

#5. Make predictions for 2021

-   In this section, the fitted Random Forest model is used to **predict
    PM10 concentrations** for the year **2021**.
-   The predictions are then added to the test data for later comparison
    with the **actual** (observed) PM10 values.
-   This procedure uses code
    from:"<https://www.statology.org/random-forest-in-r/>","<https://bradleyboehmke.github.io/HOML/random-forest.html>"

```{r}
# make predictions on the 2021 test data 

# use the fitted Random Forest model to predict PM10 values for 2021 
predictions <- predict(rf_model, newdata = test_data)

# add the predicted PM10 values to the test dataset 
test_data$predicted_pm10 <- predictions

head(test_data)


```

### Summery of Prediction Process

-   The 'predict()' function in R takes the fitted Random Forest model
    object ('rf_model') and applies it new data ('test_data').

-   'test_data\$predicted_pm10 \<- predictions' creates a new variable
    within 'test_data' dataframe that stores the model's predicted PM10
    values.

-   This step replicates the "out-of-box" prediction stage described in
    11.3: "<https://bradleyboehmke.github.io/HOML/random-forest.html>",
    where model predictions are generated through a held-out test set.

-   The resulting dataset provides us with what we need for our next
    step of Model Evaluation, where the accuracy of these predictions
    will be measured and interpreted.

#6. Evaluate Model Preformance with a metric

#7. Visualise Predictions


```{r}
# load ggplot2 for graphs 

library(ggplot2)

#scatter plot of predicted vs actual PM10 for 2021 
ggplot(test_data, aes( x = pm10_concentration, y = predicted_pm10)) +
  geom_point(colour = "steelblue", size = 3, alpha = 0.7) +
  geom_abline(slope= 1, intercept = 0, colour= "red", linetype = "dashed") +
  labs(
    title = "Predicted vs Actual PM10 concentrations (2021)",
    x = "Actual PM10 Concentration",
    y = "Predicted PM10 Concentration"
  ) + 
  theme_minimal(base_size = 13)

```
- We plot **Actual (observed) PM10 concentrations in 2021 ** against **Predicted PM10 concentrations from the Random Forest**
- The red dashed line is the "perfect prediction" line , if the model was 100% accurate , every point would lie exactly on this line. 

- What we can see: 
   - The points don't lie tightly along the 1:1 line and there is a visible vertical spread, meaning the model captures the general trend but doesn't perfectly predict PM10. 
   - There are a few points near the upper end (40-60 actual PM10), and the model's predictions for those tend to underestimate, the points for high and actual PM10 lie below the red line. 

**Maybe - analyse feauture importance**

show which variables contributed most to the models predictions

**Maybe - compare clean vs Missing data**

Run the same random forest on clean_data_europe
