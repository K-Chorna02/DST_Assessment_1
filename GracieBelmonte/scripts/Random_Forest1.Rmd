---
title: "Random Forest"
output: html_document
date: "2025-10-15"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load packages 


library(here)

library(dplyr)
 
library(tidyr) #for pivoting 

library(randomForest)


europe_data <- read.csv(here("GracieBelmonte","data","who_data.csv"))

europe_data <- europe_data %>%
  filter(who_region == "4_Eur") %>%
  select(-pm25_concentration) #we have excluded pm25 as it is too messy 

```

# 1. Introduction

[NOTES:IntroduceRandom forests

What they are Why it is appropriate How this will help answer our
prediction goal of "predicting what the PM10 value will be in 2022"

How it works - discuss mathematics behind the algorithm- TO DO \~ talk
about maths behind boot strapping How it can handle missing values ]

**what are they** The use of decision trees is both simple and intuitive.
They provide a straight forward, visual way to represent how different
variables influence a certain outcome by splitting data into small
groups based on predictor values. However, single decision trees can be
unstable and sensitive to training data. Minor changes in the dataset
can lead to major changes in the predicted outcomes. Hence this makes
them inefficient when generalising new, unseen data . #mention
over-fitting

Random Forest trees presents a way of overcoming these issues. The idea
is that they combine the predictions of many single decision trees into
one robust and accurate model. Instead of relying on a single tree, a
Random Forest builds multiple trees using different samples of the data
and random subsets of features at each split. The results from all trees
are then averaged (for regression) or voted on (for classification) to
generate the final prediction.

**why the are appropriate/ help predict** With this in mind, I can
confidently say that this model will be highly appropriate for our aim
of predicting PM10 concentrations in 2021 across European cities. Data
regarding air quality is complex. It can contain non-linear patterns,
interactions between pollutants and regional differences. A single
regression model may struggle to capture these elements, but Random
Forests can adapt to these complexities without requiring strong
assumptions about the data's structure.

reference: "<https://bradleyboehmke.github.io/HOML/random-forest.html>"

# 2. Data Preparation


NEAT:

Before training the Random Forest model, we needed to pre-process the
data to ensure it was suitable for prediction. In this section we have:

-   Identified the available years and isolate the range 2010- 2021.
    (everyone has probs done this so not just for my section)
-   Created a **training set** (2010- 2020) and a **test set** (2021).
-   Handled missing values appropriately, following the Statology (2023)
    approach.

```{r}
#check which years are avaible 
unique(europe_data$year)

# Quick summary of counts per year
table(europe_data$year)

colnames(europe_data)
```

so we can confirm the range and train on 2010-2021, \# this will probs
go in the intro as we wil all be doing the same

```{r}
# filter relevent years and variables

# do i need to use minnies combined europe data instead?

europe_pm10 <- europe_data %>%
  filter(year >= 2010 & year <= 2021) %>%
  select(country_name, year, pm10_concentration, no2_concentration, population, latitude, longitude) 

#split training and test data

train_data <- europe_pm10 %>% filter( year < 2021)
test_data <- europe_pm10 %>% filter( year == 2021)

#replace missing values in predictors with column medians 
for (i in c("no2_concentration", "population")) {
  train_data[[i]][is.na(train_data[[i]])] <- median(train_data[[i]], na.rm = TRUE)
  test_data[[i]][is.na(test_data[[i]])] <- median(test_data[[i]], na.rm = TRUE)
}

#str(train_data)

```

### Summary of Prepartion Steps

-   The dataset was filtered to include only European countries and the
    years **2010-2021**, ensuring a consistent range for analysis.

The key variables retained were: 

- `pm10_concentration` - the target variable to be predicted. 
- `no2_concentration`, `population`, `longitude` and `latitude` - predictor variables.
- `country_name` and `year` - identifying variable.


The dataset was divided into:
    -   a **training set** (2010- 2020) used to build the model, and
    -   a **test set** (2021) used to evaluate its predictive
        performance.


**Missing predictor values** were replaced using median imputation,
following the method described in:
""<https://www.statology.org/random-forest-in-r/>""


We left the target variable pm10_concentration untouched, since that is
what we are predicting. (But we did have to remove it's missing values
next step so that Random Forest could acc work?)


# 3. Train the Random Forest Model

This section implements the Random Forest algorithm, again using code
from: "<https://www.statology.org/random-forest-in-r/>"

```{r}
# random forest wont work with missing values in pm10?

train_data <- train_data %>% filter(!is.na(pm10_concentration))

#make this example reproducible - lock in the random number generators starting point 
set.seed(1)

#fit Random Forest model 
rf_model <- randomForest(
  formula = pm10_concentration ~ no2_concentration + population + latitude + longitude, 
  data = train_data
)

# view model summary 
rf_model 

```

### Summary of Model Implementation

-   The model was specified as:

**PM10 concentration** \~ **NOâ‚‚ concentration** + **population**

`pm10_concentration ~ no2_concentration + population`

-   The model was trained using **PM10 concentration** as the response
    variable, and **NO2 concentration** and **population** as predictor
    variables.
-   The Random Forest built **500 decision trees**, each trained on a
    random subset of the data and predictors.
-   Meaning the Random Forest algorithm combines the predictions of 500
    individual decision trees by averaging their outputs, a process
    known as **ensemble averaging**.
-   Thus this averaging process smooths out the variability from any one
    tree, reducing random noise and leading to a more accurate and
    stable model.

-The random seed 'set.seed(1)' ensures that results are reproducible and
so identical outcomes are produced each time the code is run.

-The model output shows: (maybe I don't need to mention this if using an
diff metric later)

- **Mean of Squared Residuals = 29.97915** -
representing the average squared difference between predicted and actual
PM10 values. 
- **% Variance Explained = 78.53%** - indicating that almost 80% of the variance in PM10 levels is explained by NO2
concentration, population, logitude and latitude. 

# 4. Make predictions for 2021

-   In this section, the fitted Random Forest model is used to **predict PM10 concentrations** for the year **2021**.
-   The predictions are then added to the test data for later comparison
    with the **actual** (observed) PM10 values.
-   This procedure uses code
    from:"<https://www.statology.org/random-forest-in-r/>","<https://bradleyboehmke.github.io/HOML/random-forest.html>"

```{r}
# make predictions on the 2021 test data 

# use the fitted Random Forest model to predict PM10 values for 2021 
predictions <- predict(rf_model, newdata = test_data)

# add the predicted PM10 values to the test dataset 
test_data$predicted_pm10 <- predictions

head(test_data)


```

### Summery of Prediction Process

-   The `predict()` function in R takes the fitted Random Forest model
    object (`rf_model`) and applies it new data (`test_data`).

-   `test_data\$predicted_pm10 \<- predictions` creates a new variable
    within 'test_data' data-frame that stores the model's predicted PM10
    values.

-   This step replicates the "out-of-box" prediction stage described in
    11.3: "<https://bradleyboehmke.github.io/HOML/random-forest.html>",
    where model predictions are generated through a held-out test set.

-   The resulting dataset provides us with what we need for our next
    step of Model Evaluation, where the accuracy of these predictions
    will be measured and interpreted.
    
### WHAT DO THESE PREDICTIONS MEAN 

- `pm10_concentration` is the actual observed PM10 value from the WHO dataset ( the "truth"), and `predicted_pm10` is the value the Random Forest model predicted using `no2_concentration`, `population`, `latitude` and `longitude` as inputs. 

Looking at these results: 

- **Spain**: Actual PM10 = 23.23 and Predicted PM10 = 22.61;
            This shows the model's prediction is very close to the true value - high accuracy. 


- **Germany**: Actual PM10 = 14.53 and Predicted PM10 = 16.57; 
            The model slightly overestimates, but still relatively close.

-**Italy**: Actual PM10 = N/A and Predicted PM10 = 25.73;
           The model produces a prediction even when the actual PM10 is missing - a key strength of the model.
           
           
-**France**: Actual PM10 = 12.15 and Predicted PM10 = 16.63; 
            The model over-predicts PM10 the most out the these 5 countries. 

# 5. Evaluate Model Preformance with a metric

# 6. Visualise Predictions

USE ANOTHER ONE FROM -GITHUB DOC 

```{r}
# load ggplot2 for graphs 

#scatter Plot 

library(ggplot2)

#scatter plot of predicted vs actual PM10 for 2021 
ggplot(test_data, aes( x = pm10_concentration, y = predicted_pm10)) +
  geom_point(colour = "steelblue", size = 3, alpha = 0.7) +
  geom_abline(slope= 1, intercept = 0, colour= "red", linetype = "dashed") +
  labs(
    title = "Predicted vs Actual PM10 concentrations (2021)",
    x = "Actual PM10 Concentration",
    y = "Predicted PM10 Concentration"
  ) + 
  theme_minimal(base_size = 13)

```


- We plot **Actual (observed) PM10 concentrations in 2021 ** against **Predicted PM10 concentrations from the Random Forest**
- The red dashed line is the "perfect prediction" line , if the model was 100% accurate , every point would lie exactly on this line. 

What we can see: 

-**Strong positive relationship**:The points generally follow a clear upward trend along the dashed red line. The model has done well at capturing the overall pattern of how PM10 varies across locations. 

-**Slight underprediction at higher PM10 levels**: For very high actual values, many points fall below the red line, indicating that model tends to underestimate extreme pollution levels. 









# 7. Analyse Feature Importance 

A useful strength of the Random Forest model is that they can provide a direct measure of **feature Importance**, in other words, quantifying how much each variable contributed to the models prediction.

This is calculated using how much each variable reduces the model's prediction error across all trees in the forest.
(Using code from lecture 6.1 OR maybe : "https://stats.stackexchange.com/questions/153663/how-to-get-the-most-important-variables-in-random-forests-in-r")


```{r}

# extract feature importance from the Random Forest Model 

importance_df <- importance(rf_model) %>%
  data.frame() %>%
  mutate(Feature = rownames(.))


#Display
colnames(importance_df)

#Plot Feature Importance 
library(ggplot2)

ggplot(importance_df,aes(x = reorder(Feature, IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.8) +
  coord_flip()+
  labs(
    title = "Feature Importance in Random Forest Mode", 
    x = "Predictor Variable",
    y = "Importance (Increase in MSE)"
  ) +
  theme_minimal(base_size = 13)




```
## Discussion of the graph 

- The x-axis represents the importance score `IncNodePurity`, which is the amount each variable improves the model's ability to make accurate splits across all trees in the Random Forest. 
- The y-axis lists the predictors: `longitude`, `latitude`, `no2_concentration` and `population`. 

### Interpretation

- `Longitude` was the most influential variable, the model relies heavily on a location's east-west position to predict PM10, suggesting that there is **strong spatial patterns** in air pollution across Europe.
- `Latitude` is the second most important variable, again suggesting that geography plays a major role in determining PM10. This could be due to climate or population distribution patterns.
- `no2_concentration` ranks third, indicating that there is clear environmental relationship, higher no2 levels would align with higher PmM10 values 
- `population` has the lowest importance score, but it would still contribute to prediction. More densely populations would have higher pollutions levels, however the other predictors would be more explanatory than population.


# 8. Conclusion 

~ wait until I have completed the metric evaluation step, then I can discuss everything. 




